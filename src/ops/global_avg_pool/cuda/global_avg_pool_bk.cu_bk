#include "../../../devices/cuda/common_cuda.h"
#include "../../utils.h"
#include "global_avg_pool.cuh"
#include <cub/block/block_reduce.cuh>

namespace infini {
    struct float2_t {
        float x, y;

        __device__ float2_t() : x(0), y(0) {}
        __device__ float2_t(int val) : x(static_cast<float>(val)), y(static_cast<float>(val)) {}
        __device__ float2_t(const float2 &val) : x(val.x), y(val.y) {}
        __device__ float2_t(const float2_t &other) : x(other.x), y(other.y) {}
        __device__ float2_t(float x, float y) : x(x), y(y) {}

        __device__ float2_t &operator=(const float2_t &other) {
            if (this != &other) {
                this->x = other.x;
                this->y = other.y;
            }
            return *this;
        }

        // __device__ float2 operator=(const int &other) const {
        //     return float2{static_cast<float>(other), static_cast<float>(other)};
        // }

        __device__ float2_t operator+(const float2_t &other) const {
            return float2_t{x + other.x, y + other.y};
        }

        __device__ float operator+(const float &other) const {
            return x + y + other;
        }

        __device__ float2_t &operator+=(const float2_t &other) {
            x += other.x;
            y += other.y;
            return *this;
        }

        __device__ float operator[](size_t index) const {
            return index == 0 ? x : y;
        }
    };

    struct half2 {
        half x, y;

        __device__ half2 &operator=(const half2 &other) {
            if (this != &other) {
                this->x = other.x;
                this->y = other.y;
            }
            return *this;
        }

        __device__ half2 &operator=(const infini::float2_t &other) {
            this->x = __float2half(other.x);
            this->y = __float2half(other.y);
            return *this;
        }

        __device__ half2 operator+(const half2 &other) const {
            return half2{__hadd(x, other.x), __hadd(y, other.y)};
        }

        __device__ half operator+(const half &other) const {
            return __hadd(__hadd(x, y), other);
        }

        __device__ half operator[](size_t index) const {
            return __hadd(x, y);
        }
    };

    struct half4 {
        __half x, y, z, w;

        __device__ half4 operator+(const half4 &other) const {
            return half4{__hadd(x, other.x), __hadd(y, other.y), __hadd(z, other.z), __hadd(w, other.w)};
        }
    };

    __device__ __forceinline__ infini::float2_t divide(infini::float2_t val, float divisor) {
        return {val.x / divisor, val.y / divisor};
    }
}// namespace infini


struct half2float_functor {
    __device__ __forceinline__ float operator()(half val) const {
        return __half2float(val);
    }
};

struct float2half_functor {
    __device__ __forceinline__ half operator()(float val) const {
        return __float2half(val);
    }
};

struct half22float_functor {
    __device__ __forceinline__ float operator()(infini::half2 val) const {
        return __half2float(val.x) + __half2float(val.y);
    }
};

struct float22half2_functor {
    __device__ __forceinline__ infini::half2 operator()(const infini::float2_t &val) const {
        return {__float2half(val.x), __float2half(val.y)};
    }
};

template<typename Tdata, typename TIdata, typename Ldata, typename FuncT2L, typename FuncTI2L>
__device__ Ldata getThreadData(const TIdata *x, uint64_t thread_idx, uint64_t block_dim, uint64_t pack_size, uint64_t idx, FuncT2L T2L, FuncTI2L TI2L) {
    if (thread_idx >= block_dim) {
        return 0;
    }
    if (thread_idx == (block_dim)) {
        auto x_ = reinterpret_cast<const TIdata *>(x);
        return TI2L(x_[idx]);
    }
    auto x_ = reinterpret_cast<const Tdata *>(x + idx);
    return T2L(*x_);
}

uint64_t getBlockDim(uint64_t size) {
    if (size < static_cast<uint64_t>(MAX_THREADS_PER_BLOCK)) {
        return size;
    }
    for (size_t i = MAX_THREADS_PER_BLOCK; i > 1; --i) {
        if (size % i == 0) {
            return i;
        }
    }
    return 1;
}

/**
 * @brief A templated vector struct that supports element-wise addition on arrays.
 *
 * @tparam T - The access data type for elements in the vector.
 * @tparam TComp - The computation data type used for arithmetic operations. 
 * @tparam N - The number of elements of type T in the vector for a single access.
 */
template<typename T, typename TComp, size_t N>
struct vecN {
    T data[N];

    __device__ __forceinline__ vecN operator+(const vecN<T, TComp, N> &other) const {
        vecN<T, TComp, N> result;

        for (int i = 0; i < N; ++i) {
            if constexpr (std::is_same<T, TComp>::value) {
                result.data[i] = data[i] + other.data[i];
            } else {
                constexpr static size_t pack_size = sizeof(T) / sizeof(TComp);
                auto data_ = reinterpret_cast<vecN<TComp, TComp, pack_size> *>(result.data);
                data_[i] = std::move(reinterpret_cast<vecN<TComp, TComp, pack_size> const *>(data)[i] +
                                     reinterpret_cast<vecN<TComp, TComp, pack_size> const *>(other.data)[i]);
            }
        }

        return result;
    }

    __device__ __forceinline__ const T &operator[](size_t i) const {
        return data[i];
    }
};

template<int BLOCK_SIZE, typename Tdata, typename LIdata, typename TIdata, typename FuncT2LI, typename FuncTI2LI>
__global__ void sum(
    LIdata *__restrict__ y,
    const Tdata *__restrict__ x,
    uint64_t data_size,
    uint64_t num_block_per_y,
    uint64_t x_per_NC_data_size,
    uint64_t offset,
    unsigned pack_size,
    FuncT2LI T2LI,
    FuncTI2LI TI2LI) {

    uint64_t x_per_NC_data_size_packed = x_per_NC_data_size / pack_size;
    uint64_t idx = blockIdx.x / num_block_per_y * x_per_NC_data_size + blockIdx.x % num_block_per_y * blockDim.x * pack_size + threadIdx.x * pack_size + offset;
    auto remainder = x_per_NC_data_size % blockDim.x;// + x_per_NC_data_size % pack_size;

    if (idx < data_size - remainder) {
        // printf("idx: %ld, %ld\n", idx, data_size - x_per_NC_data_size_packed % blockDim.x);
        // printf("idx: %ld, block: %d\n", idx, blockIdx.x);
        typedef cub::BlockReduce<LIdata, BLOCK_SIZE> BlockReduce;
        __shared__ typename BlockReduce::TempStorage temp_storage;

        LIdata thread_data = getThreadData<Tdata, TIdata, LIdata>(reinterpret_cast<TIdata const *>(x), threadIdx.x, blockDim.x, pack_size, idx, T2LI, TI2LI);
        // printf("idx: %ld, block: %d, data: %f\n", idx, blockIdx.x, thread_data);
        LIdata block_sum = BlockReduce(temp_storage).Sum(thread_data, blockDim.x);
        uint64_t idx_mod_block_dim = (idx % x_per_NC_data_size) % (blockDim.x * pack_size);

        if (idx_mod_block_dim == 0) {
            // printf("idx: %ld, block: %d\n", idx, blockIdx.x);
            if (x_per_NC_data_size > blockDim.x * pack_size && (blockIdx.x + 1) % num_block_per_y == 0) {
                // printf("idx: %ld | ", idx + blockDim.x);
                // printf("idx: %ld | r: %ld\n", idx, remainder);
                // printf("idx: %ld | block sum: %f | r: %ld\n", idx, block_sum, remainder);
                // printf("%ld\n", num_block_per_y);
                auto r_vec_size = remainder / pack_size;
                for (size_t i = 0; i < r_vec_size; ++i) {
                    auto x_TI = reinterpret_cast<const TIdata *>(x);
                    auto x_ = reinterpret_cast<Tdata const *>(x_TI + idx + (blockDim.x + i) * pack_size);
                    block_sum += T2LI(*x_);
                    // printf("blockDim.x: %ld, ", blockDim.x);
                    // printf("sum: %f, ", block_sum);
                    // printf("idx: %ld\n ", idx + blockDim.x + i);
                }
                // printf("sum: %f, ", block_sum);
                for (size_t i = 0; i < remainder % pack_size; ++i) {
                    auto x_ = reinterpret_cast<const TIdata *>(x);
                    block_sum += TI2LI(x_[idx + (blockDim.x + r_vec_size) * pack_size + i]);
                }
                // printf("\n");
            }
            // printf("idx: %ld, block: %d, data: %f\n", idx, blockIdx.x, block_sum);
            // printf("idx: %ld, sum: %f\n", idx, block_sum);
            atomicAdd(&y[idx / x_per_NC_data_size], block_sum);
            // y[idx / x_per_NC_data_size] = 1;
            // y[idx / x_per_NC_data_size] += block_sum;
        }
    }
}

template<typename Tdata, typename Ldata, typename FuncL2T>
__global__ void average(
    Tdata *__restrict__ y,
    Ldata const *__restrict__ x,
    uint64_t data_size,
    uint64_t x_per_NC_data_size,
    uint64_t offset,
    uint64_t pack_size,
    FuncL2T L2T) {
    uint64_t idx = blockIdx.x * blockDim.x + threadIdx.x + offset;
    // printf("idx: %ld, t2l: %ld, %ld, %f\n", idx, T2L(y[idx]), T2L(y[idx]) / data_size, L2T(T2L(y[idx]) / data_size));
    // printf("idx: %ld, size: %f, res: %f\n", idx, static_cast<float>(x_per_NC_data_size), __half2float(__float2half(__half2float(y[idx]) / static_cast<float>(x_per_NC_data_size))));

    if (idx < data_size) {
        // y[idx] = L2T(divide(x[idx], static_cast<Ldata>(x_per_NC_data_size)));
        y[idx] = L2T(x[idx]);
    }
}

template<typename Tdata>
__global__ void reset(
    Tdata *__restrict__ dst,
    uint64_t data_size,
    uint64_t offset,
    unsigned pack_size) {
    uint64_t idx = blockIdx.x * blockDim.x + threadIdx.x + offset;

    if (idx < data_size) {
        dst[idx] = Tdata(0);
    }
}

template<typename Tdata, typename TIdata>
void apply_reset(GlobalAvgPoolCudaDescriptor_t desc, Tdata *x, uint64_t packed_data_size, uint64_t remainder, uint64_t offset, uint64_t pack_size, cudaStream_t cuda_stream) {
    dim3 blockDims = dim3(std::max(1UL, std::min(static_cast<uint64_t>(MAX_THREADS_PER_BLOCK), packed_data_size)));
    dim3 gridDims = dim3(std::min(ROUND_UP_DIV(packed_data_size, blockDims.x), desc->max_grid_size));
    uint64_t step = gridDims.x * blockDims.x;

    for (uint64_t i = 0; i < packed_data_size; i += step) {
        reset<Tdata><<<gridDims, blockDims, 0, cuda_stream>>>(
            reinterpret_cast<Tdata *>(x), offset + desc->y_data_size, offset + i, pack_size);
    }
    if (remainder > 0) {
        blockDims = dim3(std::min(static_cast<uint64_t>(MAX_THREADS_PER_BLOCK), remainder));
        gridDims = dim3(std::min(ROUND_UP_DIV(remainder, blockDims.x), desc->max_grid_size));
        step = gridDims.x * blockDims.x;
        for (uint64_t i = 0; i < remainder; i += step) {
            reset<TIdata><<<gridDims, blockDims, 0, cuda_stream>>>(
                reinterpret_cast<TIdata *>(x), offset + desc->y_data_size, packed_data_size * pack_size + offset + i, pack_size);
        }
    }
}

template<typename Tdata, typename TIdata, typename Ldata, typename LIdata, typename FuncL2T, typename FuncLI2TI>
void apply_average(GlobalAvgPoolCudaDescriptor_t desc, void *y, void const *x, uint64_t packed_data_size, uint64_t remainder, uint64_t offset, uint64_t pack_size,
                   cudaStream_t cuda_stream, FuncL2T L2T, FuncLI2TI LI2TI) {
    dim3 blockDims = dim3(std::max(1UL, std::min(static_cast<uint64_t>(MAX_THREADS_PER_BLOCK), packed_data_size)));
    dim3 gridDims = dim3(std::min(ROUND_UP_DIV(packed_data_size, blockDims.x), desc->max_grid_size));
    uint64_t step = gridDims.x * blockDims.x;

    for (uint64_t i = 0; i < packed_data_size; i += step) {
        average<Tdata, Ldata><<<gridDims, blockDims, 0, cuda_stream>>>(
            reinterpret_cast<Tdata *>(y), reinterpret_cast<Ldata const *>(x), offset + desc->y_data_size, desc->x_per_NC_data_size, offset + i, pack_size, L2T);
    }

    if (remainder > 0) {
        blockDims = dim3(std::min(static_cast<uint64_t>(MAX_THREADS_PER_BLOCK), remainder));
        gridDims = dim3(std::min(ROUND_UP_DIV(remainder, blockDims.x), desc->max_grid_size));
        step = gridDims.x * blockDims.x;
        for (uint64_t i = 0; i < remainder; i += step) {
            average<TIdata, LIdata><<<gridDims, blockDims, 0, cuda_stream>>>(
                reinterpret_cast<TIdata *>(y), reinterpret_cast<LIdata const *>(x), offset + desc->y_data_size, desc->x_per_NC_data_size, packed_data_size * pack_size + offset + i, pack_size, LI2TI);
        }
    }
}


template<typename Tdata, typename TIdata, typename Ldata, typename LIdata,
         typename FuncT2LI, typename FuncL2T, typename FuncTI2LI, typename FuncLI2TI, typename Div>
void global_avg_pool_nv_gpu(GlobalAvgPoolCudaDescriptor_t desc, Ldata *workspace, Tdata *y, Tdata const *x,
                            uint64_t data_size, uint64_t pack_size, uint64_t offset,
                            FuncT2LI T2LI, FuncTI2LI TI2LI, FuncL2T L2T, FuncLI2TI LI2TI, Div divide, void *stream) {
    if (data_size == 0) {
        return;
    }

    auto y_packed_size = desc->y_data_size / pack_size;
    auto y_remainder = desc->y_data_size % pack_size;
    // printf("%ld, %ld\n", y_packed_size, y_remainder);

    cudaStream_t cuda_stream = reinterpret_cast<cudaStream_t>(stream);

    apply_reset<Ldata, LIdata>(desc, workspace, y_packed_size, y_remainder, offset, pack_size, cuda_stream);

    // dim3 blockDims = dim3(std::min(static_cast<uint64_t>(MAX_THREADS_PER_BLOCK), data_size));
    // dim3 blockDims = dim3(MAX_THREADS_PER_BLOCK);
    // dim3 blockDims = dim3(getBlockDim(desc->x_per_NC_data_size));
    auto x_packed_size = desc->x_per_NC_data_size / pack_size;
    dim3 blockDims = dim3(std::min(static_cast<uint64_t>(4), x_packed_size));
    dim3 gridDims = dim3(std::min(ROUND_UP_DIV(data_size, pack_size) / blockDims.x, desc->max_grid_size));
    uint64_t step = gridDims.x * blockDims.x;

    // printf("grid: %d, block: %d\n", gridDims.x, blockDims.x);
    // printf("grid_y: %d, block_y: %d\n", gridDims_y.x, blockDims_y.x);

    for (uint64_t i = 0; i < x_packed_size; i += step) {
        // printf("x_packed_size: %ld, step: %ld\n", x_packed_size, step);
        sum<MAX_THREADS_PER_BLOCK, Tdata, LIdata, TIdata><<<gridDims, blockDims, 0, cuda_stream>>>(
            reinterpret_cast<LIdata *>(workspace), reinterpret_cast<Tdata const *>(x), offset + data_size, ROUND_UP_DIV(x_packed_size, blockDims.x), desc->x_per_NC_data_size, offset + 0, pack_size, T2LI, TI2LI);
    }

    // blockDims_y = dim3(std::min(static_cast<uint64_t>(MAX_THREADS_PER_BLOCK), desc->y_data_size));
    // gridDims_y = dim3(std::min(ROUND_UP_DIV(desc->y_data_size, blockDims_y.x), desc->max_grid_size));
    // step_y = gridDims_y.x * blockDims_y.x;
    // for (uint64_t i = 0; i < desc->y_data_size; i += step_y) {
    //     average<TIdata, LIdata><<<gridDims_y, blockDims_y, 0, cuda_stream>>>(
    //         reinterpret_cast<TIdata *>(y), reinterpret_cast<LIdata *>(workspace), offset + desc->y_data_size, desc->x_per_NC_data_size, offset + i, pack_size, LI2TI);
    // }

    apply_average<Tdata, TIdata, Ldata, LIdata>(desc, y, workspace, y_packed_size, y_remainder, offset, pack_size, cuda_stream, L2T, LI2TI);
}

infiniopStatus_t global_avg_pool_nv_gpu_f16(GlobalAvgPoolCudaDescriptor_t desc, void *workspace, uint64_t workspace_size, void *y, void const *x, void *stream) {
    // use cuDNN lib
    if (desc->ndim <= 4) {
        checkCudnnError(use_cudnn(desc->cudnn_handles_t, desc->device_id,
                                  [&](cudnnHandle_t handle) { return cudnnPoolingForward(handle, desc->pool_desc,
                                                                                         &desc->alpha, desc->x_desc, x, &desc->beta,
                                                                                         desc->y_desc, y); }));
    } else {
        auto data_size = desc->y_data_size * desc->x_per_NC_data_size;
        auto x_half2 = reinterpret_cast<const infini::half2 *>(x);
        auto y_half2 = reinterpret_cast<infini::half2 *>(y);
        auto workspace_ = reinterpret_cast<infini::float2_t *>(workspace);
        half2float_functor half_to_float;
        half22float_functor half2_to_float;
        float22half2_functor float2_to_half2;
        float2half_functor float_to_half;
        global_avg_pool_nv_gpu<infini::half2, half, infini::float2_t, float>(desc, workspace_, y_half2, x_half2, data_size, 2, 0, half2_to_float, half_to_float, float2_to_half2, float_to_half, infini::divide, stream);
    }

    cudaDeviceSynchronize();
    return STATUS_SUCCESS;
}

infiniopStatus_t cudaGlobalAvgPool(GlobalAvgPoolCudaDescriptor_t desc,
                                   void *workspace, uint64_t workspace_size,
                                   void *y, void const *x,
                                   void *stream) {
    if (desc->dtype == F16) {
        checkCudaError(cudaSetDevice(desc->device_id));
        return global_avg_pool_nv_gpu_f16(desc, workspace, workspace_size, y, x, stream);
    }
    return STATUS_BAD_TENSOR_DTYPE;
}
